
# Big Data - Nuclear Reactor Status Prediction (CMP3749M_Assessment_02)


## Overview

Train a decision tree, a support vector machine model and an artificial neural network to predict the status of nuclear reactors based on previous data. Make an analysis over a data set to guide the stakeholders to understand the data.


## Dataset Information

It is a dataset of pressurised water reactors (a type of nuclear reactors) with various measurements in different parts of reactors, including vibration, pressure and power levels. The first column in the spreadsheet indicates the status of reactors, i.e. ‘normal’ or ‘abnormal’. All the other columns are features which could help us to gain insights in the status of reactors.
   
   
## Summary of Implementation
### Data Summary, Understanding and Visualisation

Once the data was determined to have no missing values, statistical analysis was to be performed to get a better understanding. The ‘readtable’ function is used to read the data into a table variable where it can be dealt with inside MATLAB. Next, the two groups are separated by accessing the unique words ‘Normal’ and ‘Abnormal’ and assigned each one, respectively, to the correct group.

Once the strings have been used to assign the numerical data to the right group, they are removed from the table. This is done to allow the table variable to be converted to a cell array and then from a cell array to a matrix, which is done by using built-in functions. A matrix is used as the primary data store when dealing with all the data as it is easily manipulated for future tasks unless another variable type is needed.

To find the summary statistics for all the features, various function files were created to perform calculations more efficiently. These can be seen inside MATLAB with names following the ‘nuclearX.m’ naming structure. The number of elements in the initial unsorted array are first counted to initialise the for loop. It begins by comparing the values in the first and second locations; if the value in the first position is less than that in the second, those values are swapped using a temp variable, and the loop begins again. This is repeated until the array can be sorted through with all the smaller values being greater than the ones below them. The first and last values of the feature selected become the minimum and maximum of that particular feature as well as the median being found by finding the length of the feature (number of elements in the array) and diving by two to reach the central value, which is then used in a loop to access that appropriate value.

The mode, mean, and variance all require their own functions to calculate the results. An example of how the variance is calculated can be seen in the ‘nuclearVariance’ function file. This function takes the inputs of the chosen feature column and the previously calculated mean value for that feature. The function then finds the distance between these two values and squares that calculated distance. These distances are finally totalled, and the variance is found by dividing this number by the total elements minus one of the feature and passed as the output to be called in the main file. The other functions can be seen inside the supporting code, and all follow basic formulae to calculate the results.

Furthermore, to find the relationship between each of the features, the Pearson’s product-moment correlation coefficient (PMCC) needs to be calculated for all the variables. The two features are parsed as arguments into the function as ‘column1’ and ‘column2’, then the formula for finding the Sxy, Sxx, and Syy values is calculated, and the final PMCC value is found and returned.


### Classification & Big data Analysis

The data is required to be split into two segments, 70% of the data being for training and the rest, 30%, for testing once training has been performed. The original data has 996 rows of values, excluding the headers. 70% of this is 697, which is then parsed into a loop to take out 697 rows of values. This must be performed on entire rows, not separately; this would mix the corresponding values and create a completely random dataset. 

Additionally, as the data is shuffled randomly each time the program is compiled, the amount of abnormal and normal values in both the training and testing data will differ. A small section of code was written at the end of the ‘splitAndTrain.m’ file to count the amount of each value in training and testing. As of the current compiled version of code used while writing this section of the report, there are 354 normal status reactors and 343 under abnormal status being used for training data. While there are equal amounts of normal and abnormal status reactors in the original dataset, a slight skew when shuffling randomly is expected, explaining the higher values of normal in this iteration of compilation. Furthermore, there are 144 normal and 155 abnormal status reactors in the testing data, which is again expected as there was less abnormal assigned to the training data.

Moreover, this meets the requirements of a 70-30 split, being 687 values in training and 299 values being used for testing. However, an arguably better approach to shuffle the data would shuffle all of the normal values between each other and all the abnormal values separately; once done they could be equally split so there always the same amount of each in both testing and training. This may cause slightly more accuracy when it comes to testing, as the testing set already expects an equal amount of values being normal and abnormal. However, this may also increase the model’s artificiality as there is less randomness and noise when shuffling, which may be beneficial when trying to train for a real-world situation.

Before the first classification model could be trained, additional data pre-processing needed to be carried out. This involved splitting the status column away from the rest of the numerical data and formatting the X data (numerical data) into a matrix. Furthermore, this method was also applied to the testing dataset to prepare it for testing. To train the decision tree, the built-in function ‘fitctree’ was used. Doing so and inputting the appropriate data returns a fitted binary classification decision tree, which has been trained to predict the nuclear reactor’s status based on the numeric data. The ‘fitctree’ method can also be used for multiclass classification other than just the status variables (MathWorks, 2020). The ‘predict’ function is used with the parameters of ‘mdl’, which is the trained decision tree model from above, and ‘modelTestingDataInput’, which is the numerical values from the testing dataset. This line of code will output the new class labels correlating to the test set labels, which are compared and discussed in a later section.

The next model that was trained was the support vector machine model. This model is trained using the ‘fitcsvm’ function with the parameters of the X and Y data as well as other additional input arguments. The data being used to train and predict the new class labels is the same as that of the decision tree. The model was first trained using no additional input arguments to see the performance output, which had an error rate of around 25%, which was not ideal for classification. Different parameters were then added to the model, and through the process of trial and error, the best resultant performance for the model was found.

Firstly, the data was standardised, which is the process of making sure all the numerical data is consistent. MATLAB performs this by scaling the data based on the weighted column mean and standard deviation. This change was the one that affected performance the most by taking the average error rate from around 25% to as low as 8% in some iterations. Next, the ‘KernelFunction’ was set to ‘rbf’, which stands for radial basis function. Kernel functions are often used in statistics as weighting functions for non-parametric estimation techniques (V. A. Epanechnikov, 1967). The radial basis function was chosen as again this was found to have the best performance out of all the functions that could have been selected. The RBF function is one of the most generalised forms of kernelization.

When the formula is used for two data points, the similarity between them is calculated, where one will occur when the two points are exactly the same, meaning they are very similar, and the closer to zero, the further apart they are (dissimilar). Finally, the ‘KernalScale’ function is set to ‘auto’ rather than the default value of one. By setting this parameter to automatic, MATLAB selects the appropriate scale factor for the data using a heuristic procedure. This combination of input arguments was found to produce the best performance for the support vector machine model.

The final model that was trained was an artificial neural network (ANN) model. Before this model can be trained, additional data pre-processing was explicitly required for the ANN. The status column was required to be converted to binary to be accepted by the ANN model. To achieve this, the code loops through the dataset and turns any cell that is ‘Normal’ to the number one and ‘Abnormal’ to a zero. Once the data has been prepared to be parsed into the model, a feedforward neural network with one hidden layer of size ten is created. A feedforward model was one of the first and most straightforward types of artificial neural networks created (Schmidhuber, 2015), but for the chosen dataset, was found the be the most efficient. ‘net’ is simply the previously defined network, ‘inputs’ is the entire numerical data, previously named ‘modelTrainingDataX‘, and ‘targets’ is the now binary version of the status column.

